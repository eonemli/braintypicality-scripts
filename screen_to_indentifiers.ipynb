{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column =================== scrn_asd\n",
      "Total non-null: 11828\n",
      "scrn_asd\n",
      "0.0    11627\n",
      "1.0      201\n",
      "NaN       39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column =================== scrn_asd_regclasses\n",
      "Total non-null: 201\n",
      "scrn_asd_regclasses\n",
      "NaN    11666\n",
      "1.0      195\n",
      "0.0        6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column =================== scrn_sud\n",
      "Total non-null: 11830\n",
      "scrn_sud\n",
      "0.0    11829\n",
      "NaN       37\n",
      "1.0        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved 195 passing keys to 'abcd_qc_passing_keys.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('abcd_p_screen.csv')\n",
    "\n",
    "# Print total and value counts for each target column\n",
    "for col in ['scrn_asd', 'scrn_asd_regclasses', 'scrn_sud']:\n",
    "\n",
    "    print(f'\\nColumn =================== {col}')\n",
    "    print('Total non-null:', df[col].notnull().sum())\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "\n",
    "# Function to get src_subject_id keys where all given columns == 1\n",
    "def get_keys_all_ones(df, columns):\n",
    "    mask = (df[columns] == 1).all(axis=1)\n",
    "    return df.loc[mask, 'src_subject_id'].dropna().unique()\n",
    "\n",
    "# Example usage â€” you can change the list below to any columns you want\n",
    "target_columns = ['scrn_asd', 'scrn_asd_regclasses']#, 'scrn_sud']\n",
    "passing_keys = get_keys_all_ones(df, target_columns)\n",
    "\n",
    "# Save the result to a txt file\n",
    "with open('abcd_qc_passing_keys_195.txt', 'w') as f:\n",
    "    for key in passing_keys:\n",
    "        f.write(f\"{key}\\n\")\n",
    "\n",
    "print(f\"\\nSaved {len(passing_keys)} passing keys to 'abcd_qc_passing_keys.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file saved as: abcd_qc_passing_keys_195_no_undersc.txt\n"
     ]
    }
   ],
   "source": [
    "# Define input and output file names\n",
    "input_filename = \"abcd_qc_passing_keys_195.txt\"\n",
    "output_filename = \"abcd_qc_passing_keys_195_no_undersc.txt\"\n",
    "\n",
    "# Read the file and remove underscores from each line\n",
    "with open(input_filename, \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "lines_no_underscore = [line.replace(\"_\", \"\") for line in lines]\n",
    "\n",
    "# Write the modified lines to the new file\n",
    "with open(output_filename, \"w\") as f:\n",
    "    for line in lines_no_underscore:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Modified file saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IDs in CSV: 11867\n",
      "Total IDs in Data folder: 3884\n",
      "IDs in both CSV and Data: 3877\n",
      "IDs in CSV but NOT in Data: 7990\n",
      "IDs in Data but NOT in CSV: 7\n",
      "Common IDs with both T1w and T2w: 3781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === Load abcd_p_screen.csv and extract src_subject_id ===\n",
    "df = pd.read_csv('abcd_p_screen.csv')\n",
    "csv_ids_raw = df['src_subject_id'].dropna().unique()\n",
    "csv_ids = set(id_.replace('_', '') for id_ in csv_ids_raw)\n",
    "\n",
    "print(f\"Total IDs in CSV: {len(csv_ids)}\")\n",
    "\n",
    "# === Extract NDARINV IDs from folder names ===\n",
    "data_folder = '/BEE/Connectome/ABCD/ImageData/Data'\n",
    "data_ids = set()\n",
    "folder_paths = {}\n",
    "\n",
    "for name in os.listdir(data_folder):\n",
    "    match = re.match(r\"sub-(NDARINV[0-9A-Z]+)\", name)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        data_ids.add(subject_id)\n",
    "        folder_paths[subject_id] = os.path.join(data_folder, name)\n",
    "\n",
    "print(f\"Total IDs in Data folder: {len(data_ids)}\")\n",
    "\n",
    "# === Compare sets ===\n",
    "common_ids = csv_ids & data_ids\n",
    "only_in_csv = csv_ids - data_ids\n",
    "only_in_data = data_ids - csv_ids\n",
    "\n",
    "print(f\"IDs in both CSV and Data: {len(common_ids)}\")\n",
    "print(f\"IDs in CSV but NOT in Data: {len(only_in_csv)}\")\n",
    "print(f\"IDs in Data but NOT in CSV: {len(only_in_data)}\")\n",
    "\n",
    "# === Check T1w and T2w availability among common IDs ===\n",
    "count_with_t1_and_t2 = 0\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    anat_path = os.path.join(folder_paths[subject_id], 'ses-baselineYear1Arm1', 'anat')\n",
    "    has_t1w = has_t2w = False\n",
    "\n",
    "    if os.path.isdir(anat_path):\n",
    "        for file in os.listdir(anat_path):\n",
    "            if re.search(r'_T1w\\.nii\\.gz$', file):\n",
    "                has_t1w = True\n",
    "            if re.search(r'_T2w\\.nii\\.gz$', file):\n",
    "                has_t2w = True\n",
    "\n",
    "    if has_t1w and has_t2w:\n",
    "        count_with_t1_and_t2 += 1\n",
    "\n",
    "print(f\"Common IDs with both T1w and T2w: {count_with_t1_and_t2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IDs in abcd_qc_passing_keys.txt: 195\n",
      "Total IDs in Data folder: 3884\n",
      "IDs in both TXT and Data: 41\n",
      "IDs in TXT but NOT in Data: 154\n",
      "IDs in Data but NOT in TXT: 3843\n",
      "Common IDs with both T1w and T2w: 40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# === Load abcd_qc_passing_keys.txt and normalize IDs ===\n",
    "with open(\"abcd_qc_passing_keys.txt\", \"r\") as f:\n",
    "    txt_ids_raw = [line.strip() for line in f if line.strip()]\n",
    "txt_ids = set(id_.replace('_', '') for id_ in txt_ids_raw)\n",
    "\n",
    "print(f\"Total IDs in abcd_qc_passing_keys.txt: {len(txt_ids)}\")\n",
    "\n",
    "# === Extract NDARINV IDs from folder names ===\n",
    "data_folder = '/BEE/Connectome/ABCD/ImageData/Data'\n",
    "data_ids = set()\n",
    "folder_paths = {}\n",
    "\n",
    "for name in os.listdir(data_folder):\n",
    "    match = re.match(r\"sub-(NDARINV[0-9A-Z]+)\", name)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        data_ids.add(subject_id)\n",
    "        folder_paths[subject_id] = os.path.join(data_folder, name)\n",
    "\n",
    "print(f\"Total IDs in Data folder: {len(data_ids)}\")\n",
    "\n",
    "# === Compare sets ===\n",
    "common_ids = txt_ids & data_ids\n",
    "only_in_txt = txt_ids - data_ids\n",
    "only_in_data = data_ids - txt_ids\n",
    "\n",
    "print(f\"IDs in both TXT and Data: {len(common_ids)}\")\n",
    "print(f\"IDs in TXT but NOT in Data: {len(only_in_txt)}\")\n",
    "print(f\"IDs in Data but NOT in TXT: {len(only_in_data)}\")\n",
    "\n",
    "# === Check T1w and T2w availability among common IDs ===\n",
    "count_with_t1_and_t2 = 0\n",
    "\n",
    "for subject_id in common_ids:\n",
    "    anat_path = os.path.join(folder_paths[subject_id], 'ses-baselineYear1Arm1', 'anat')\n",
    "    has_t1w = has_t2w = False\n",
    "\n",
    "    if os.path.isdir(anat_path):\n",
    "        for file in os.listdir(anat_path):\n",
    "            if re.search(r'_T1w\\.nii\\.gz$', file):\n",
    "                has_t1w = True\n",
    "            if re.search(r'_T2w\\.nii\\.gz$', file):\n",
    "                has_t2w = True\n",
    "\n",
    "    if has_t1w and has_t2w:\n",
    "        count_with_t1_and_t2 += 1\n",
    "\n",
    "print(f\"Common IDs with both T1w and T2w: {count_with_t1_and_t2}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Save the result to a txt file\n",
    "# with open('missing_qc_ids_in_data.txt', 'w') as f:\n",
    "#     for key in only_in_txt:\n",
    "#         f.write(f\"{key}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column =================== scrn_asd\n",
      "Total non-null: 11828\n",
      "scrn_asd\n",
      "0.0    11627\n",
      "1.0      201\n",
      "NaN       39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column =================== scrn_asd_regclasses\n",
      "Total non-null: 201\n",
      "scrn_asd_regclasses\n",
      "NaN    11666\n",
      "1.0      195\n",
      "0.0        6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column =================== scrn_sud\n",
      "Total non-null: 11830\n",
      "scrn_sud\n",
      "0.0    11829\n",
      "NaN       37\n",
      "1.0        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initial passing keys from CSV (all columns == 1): 195\n",
      "\n",
      "Final QC-passing keys with both T1w and T2w: 40\n",
      "Saved to 'abcd_qc_passing_keys.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('abcd_p_screen.csv')\n",
    "\n",
    "# Print total and value counts for each target column\n",
    "for col in ['scrn_asd', 'scrn_asd_regclasses', 'scrn_sud']:\n",
    "    print(f'\\nColumn =================== {col}')\n",
    "    print('Total non-null:', df[col].notnull().sum())\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "\n",
    "# Function to get src_subject_id keys where all given columns == 1\n",
    "def get_keys_all_ones(df, columns):\n",
    "    mask = (df[columns] == 1).all(axis=1)\n",
    "    return df.loc[mask, 'src_subject_id'].dropna().unique()\n",
    "\n",
    "# Step 1: Get passing keys from screening CSV\n",
    "target_columns = ['scrn_asd', 'scrn_asd_regclasses']  # Change if needed\n",
    "passing_keys = get_keys_all_ones(df, target_columns)\n",
    "print(f\"\\nInitial passing keys from CSV (all columns == 1): {len(passing_keys)}\")\n",
    "\n",
    "# Step 2: Normalize keys (remove _ to match folder names)\n",
    "passing_keys_normalized = {k.replace('_', ''): k for k in passing_keys}\n",
    "\n",
    "# Step 3: Check which have both T1w and T2w images\n",
    "data_folder = '/BEE/Connectome/ABCD/ImageData/Data'\n",
    "folder_paths = {}\n",
    "\n",
    "for name in os.listdir(data_folder):\n",
    "    match = re.match(r\"sub-(NDARINV[0-9A-Z]+)\", name)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        folder_paths[subject_id] = os.path.join(data_folder, name)\n",
    "\n",
    "# Filter those with both T1w and T2w\n",
    "final_keys = []\n",
    "\n",
    "for normalized_id, original_id in passing_keys_normalized.items():\n",
    "    subject_path = folder_paths.get(normalized_id)\n",
    "    if not subject_path:\n",
    "        continue  # Subject folder not found\n",
    "\n",
    "    anat_path = os.path.join(subject_path, 'ses-baselineYear1Arm1', 'anat')\n",
    "    has_t1w = has_t2w = False\n",
    "\n",
    "    if os.path.isdir(anat_path):\n",
    "        for file in os.listdir(anat_path):\n",
    "            if re.search(r'_T1w\\.nii\\.gz$', file):\n",
    "                has_t1w = True\n",
    "            if re.search(r'_T2w\\.nii\\.gz$', file):\n",
    "                has_t2w = True\n",
    "\n",
    "    if has_t1w and has_t2w:\n",
    "        final_keys.append(original_id)\n",
    "\n",
    "# Step 4: Save the filtered keys\n",
    "with open('abcd_qc_passing_keys.txt', 'w') as f:\n",
    "    for key in sorted(final_keys):\n",
    "        f.write(f\"{key}\\n\")\n",
    "\n",
    "print(f\"\\nFinal QC-passing keys with both T1w and T2w: {len(final_keys)}\")\n",
    "print(\"Saved to 'abcd_qc_passing_keys.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is for the new ABCD dataz copied from Longleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keys (original lines): 195\n",
      "Unique normalized keys: 195\n",
      "Total subject folders in /BEE/Connectome/ABCD/ImageData/Data_abcd_asd_scr_pos: 161\n",
      "\n",
      "=== Comparison Results ===\n",
      "Keys found in data: 161\n",
      "Keys missing in data: 34\n",
      "Data folders not in key list: 0\n",
      "\n",
      "Saved 161 found keys to 'abcd_qc_passing_keys_161.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# ----------------------\n",
    "# 1) Load the TXT file\n",
    "# ----------------------\n",
    "input_txt = \"abcd_qc_passing_keys_195.txt\"\n",
    "with open(input_txt, \"r\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Keep a map of \"normalized_key\" -> \"original_key_with_underscores\"\n",
    "normalized_to_original = {}\n",
    "for line in lines:\n",
    "    norm = line.replace(\"_\", \"\")\n",
    "    normalized_to_original[norm] = line\n",
    "\n",
    "# We only need the normalized set for matching\n",
    "keys_normalized_set = set(normalized_to_original.keys())\n",
    "print(f\"Total keys (original lines): {len(lines)}\")\n",
    "print(f\"Unique normalized keys: {len(keys_normalized_set)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Gather subject folders in Data_abcd_asd_scr_pos\n",
    "# --------------------------------------------------\n",
    "data_folder = \"/BEE/Connectome/ABCD/ImageData/Data_abcd_asd_scr_pos\"\n",
    "folder_names = os.listdir(data_folder)\n",
    "\n",
    "# We'll extract the NDARINV ID from folder names like: sub-NDARINVXXXXXX\n",
    "data_ids = set()\n",
    "for name in folder_names:\n",
    "    match = re.match(r\"sub-(NDARINV[0-9A-Z]+)\", name)\n",
    "    if match:\n",
    "        data_ids.add(match.group(1))\n",
    "\n",
    "print(f\"Total subject folders in {data_folder}: {len(data_ids)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Compare sets\n",
    "# --------------------------------------------------\n",
    "common_ids = keys_normalized_set & data_ids\n",
    "missing_in_data = keys_normalized_set - data_ids\n",
    "extra_in_data = data_ids - keys_normalized_set\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Print counts\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== Comparison Results ===\")\n",
    "print(f\"Keys found in data: {len(common_ids)}\")\n",
    "print(f\"Keys missing in data: {len(missing_in_data)}\")\n",
    "print(f\"Data folders not in key list: {len(extra_in_data)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) Save the found keys (in original underscore format)\n",
    "# --------------------------------------------------\n",
    "found_count = len(common_ids)\n",
    "output_txt = f\"abcd_qc_passing_keys_{found_count}.txt\"\n",
    "\n",
    "with open(output_txt, \"w\") as f:\n",
    "    # Sort for consistency\n",
    "    for norm_id in sorted(common_ids):\n",
    "        original_line = normalized_to_original[norm_id]\n",
    "        f.write(original_line + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved {found_count} found keys to '{output_txt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved modified keys to abcd_qc_passing_keys_161_no_undersc.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output filenames\n",
    "input_file = \"abcd_qc_passing_keys_161.txt\"\n",
    "output_file = \"abcd_qc_passing_keys_161_no_undersc.txt\"\n",
    "\n",
    "# Read the input file and remove underscores from each line\n",
    "with open(input_file, \"r\") as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for line in lines:\n",
    "        # Remove leading/trailing whitespace and underscores, then write to file\n",
    "        new_line = line.strip().replace(\"_\", \"\")\n",
    "        outfile.write(new_line + \"\\n\")\n",
    "\n",
    "print(f\"Saved modified keys to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function brain_extraction in module antspynet.utilities.brain_extraction:\n",
      "\n",
      "brain_extraction(image, modality, verbose=False)\n",
      "    Perform brain extraction using U-net and ANTs-based training data.  \"NoBrainer\"\n",
      "    is also possible where brain extraction uses U-net and FreeSurfer training data\n",
      "    ported from the\n",
      "\n",
      "    https://github.com/neuronets/nobrainer-models\n",
      "\n",
      "    Arguments\n",
      "    ---------\n",
      "    image : ANTsImage\n",
      "        input image (or list of images for multi-modal scenarios).\n",
      "\n",
      "    modality : string\n",
      "        Modality image type.  Options include:\n",
      "            * \"t1\": T1-weighted MRI---ANTs-trained.  Previous versions are specified as \"t1.v0\", \"t1.v1\".\n",
      "            * \"t1nobrainer\": T1-weighted MRI---FreeSurfer-trained: h/t Satra Ghosh and Jakub Kaczmarzyk.\n",
      "            * \"t1combined\": Brian's combination of \"t1\" and \"t1nobrainer\".  One can also specify\n",
      "                            \"t1combined[X]\" where X is the morphological radius.  X = 12 by default.\n",
      "            * \"t1threetissue\":  T1-weighted MRI---originally developed from BrainWeb20 (and later expanded).\n",
      "                                Label 1:  parenchyma, label 2: meninges/csf, label 3: misc. head.\n",
      "            * \"flair\": FLAIR MRI.   Previous versions are specified as \"flair.v0\".\n",
      "            * \"t2\": T2 MRI.  Previous versions are specified as \"t2.v0\".\n",
      "            * \"t2star\": T2Star MRI.\n",
      "            * \"bold\": 3-D mean BOLD MRI.  Previous versions are specified as \"bold.v0\".\n",
      "            * \"fa\": fractional anisotropy.  Previous versions are specified as \"fa.v0\".\n",
      "            * \"mra\": MRA h/t Tyler Hanson \"mmbop\".\n",
      "            * \"t1t2infant\": Combined T1-w/T2-w infant MRI h/t Martin Styner.\n",
      "            * \"t1infant\": T1-w infant MRI h/t Martin Styner.\n",
      "            * \"t2infant\": T2-w infant MRI h/t Martin Styner.\n",
      "\n",
      "    verbose : boolean\n",
      "        Print progress to the screen.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    ANTs probability brain mask image.\n",
      "\n",
      "    Example\n",
      "    -------\n",
      "    >>> probability_brain_mask = brain_extraction(brain_image, modality=\"t1\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from antspynet import brain_extraction\n",
    "help(brain_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ants: 0.5.4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'antspynet' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mantspynet\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mants:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ants\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantspynet:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mantspynet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'antspynet' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import ants\n",
    "import antspynet\n",
    "print(\"ants:\", ants.__version__)\n",
    "print(\"antspynet:\", antspynet.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ASD2/emre_projects/miniconda3/envs/myenv/lib/python3.12/site-packages/ants/__init__.py\n",
      "0.5.4\n"
     ]
    }
   ],
   "source": [
    "import ants\n",
    "print(ants.__file__)\n",
    "print(ants.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
